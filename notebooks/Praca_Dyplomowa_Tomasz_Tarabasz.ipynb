{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Praca_Dyplomowa_Tomasz_Tarabasz.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2APrRHXAtumo",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1MjzUke5SDxffqmpruiUzfdBLR6gKbCBM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf-4ZEZBc141",
        "colab_type": "text"
      },
      "source": [
        "# [Analiza problemu reidentyfikacji obiektów z wykorzystaniem głębokich sieci neutronowych.](Praca_Dyplomowa_Tomasz_Tarabasz.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQo84u2Gd5ZK",
        "colab_type": "text"
      },
      "source": [
        "## [1. Streszczenie](notebook/01_Streszczenie.*ipnb*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sBEk_sYeSx4",
        "colab_type": "text"
      },
      "source": [
        "Praca zawiera opis oraz definicję problemu reidentyfikacji obrazów. W dokumencie przedstawiono wykorzystanie dwóch powszechnie wykorzystywanych systemów do testowania własnych rozwiązań problemu reidentyfikacji jak i modyfikacji obecnie wiodących architektur. W pracy przedstawiono przykład zastosowania systemu z w połączeniu z systemem detekcji osób. cd ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg_BWd6QfN7-",
        "colab_type": "text"
      },
      "source": [
        "## [2. Wstęp]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbv-u4UbwPk6",
        "colab_type": "text"
      },
      "source": [
        "### 1. Motywacja "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ_RrIJWwfDh",
        "colab_type": "text"
      },
      "source": [
        "Problem rozpoznawania osób i obiektów na różnych ujeciach z wielu kamer stał się w ostatnim czasie jednym z najczęściej badanych zagadnień. Potwierdza to cytac z pracy [[4] Torchreid: Library for Deep Learning Person Re_Identyfication in Pytoch](https://arxiv.org/pdf/1910.10093.pdf) \"Driven by the growing demands for intelligent surveillance and forensic applications, person re-identification (re-ID) has become a topical\n",
        "research area in computer vision.\" w wolnym tłumaczeniu \"Rozwiązanie umożliwiające identyfikację osób z różnych kamer oraz ujęć stało się jednym z najczęściej badanych zagadnień. Stało się tak za sprawą rosnącego zainteresowania służ wykorzstaniem aplikacyjnym takiego rozwiązania\". Na potwierdzenie tego wniosku przedstawiono wykres obrazujący ilość \n",
        "<br><br>\n",
        "<img src=\"https://github.com/tomektarabasz/Praca_Dyplomowa_Tomasz_Tarabasz/blob/master/img/IloscPrac.jpg?raw=true\" alt=\"drawing\" width=\"400\"/>\n",
        "<br><br>\n",
        "\n",
        "Potencjał rozwiązania tego zagadnienia wykracza jednak poza użycie jakim są zainteresowane służby bezpieczeństwa. Jest zdecydowanie bardziej ogólnym problemem dającym możliwość przypisywania identyfikatora dla obiektów tej samej klasy. Uogólnia zatem temat detekcji oraz rozszerza dziedzinę jej rozwiązań o klasy pośrednie w stusunku do wykorzystanych w procesie uczenia. \n",
        "<br><br>\n",
        "\n",
        "Warto również zaznaczyć, że zgodnie z - [[6] reid-strong-baseline - pdf](https://arxiv.org/pdf/1903.07071.pdf) znaczący postęp w tej tematycie dokonał się właśnie dzięki GSN (głębokim siecią neuronowym). Odnosząć się do cytatu \"Person re-identification (ReID) with deep neural networks has made progress and achieved high performance\n",
        "in recent years. However, many state-of-the-arts methods\n",
        "design complex network structure and concatenate multibranch features. In the literature, some effective training\n",
        "tricks or refinements are briefly appeared in several papers\n",
        "or source codes\" należy zauważyć, że zgodnie z przytoczonym fragmentem rozwój w tej dziedzinie jest bardzo szybki i istnieje przestrzeń na łaczenie wielu proponowanych rozwiązń."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZhuT5xw1fVU",
        "colab_type": "text"
      },
      "source": [
        "### 2. Cel pracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C3jIXQ_1lIm",
        "colab_type": "text"
      },
      "source": [
        "Dalsza cześć pracy wymaga sformułowania założeń. Te natomiast wymagaja postawienia celi jakie adresować będzie ta praca. Zawierto je w dwóch punktach:\n",
        "\n",
        "*   Modyfikacja istniejących rozwiązań z założeniem poprawy wyników.\n",
        "*   Wykorzystnie rozwiązani jako cześci systemu (link do opisu systemu)\n",
        "\n",
        "Założenia czynione w dalszej części pracy będą wynikały z powyższych punktów.\n",
        "<br><br>\n",
        "Pierwszy z nich odnosi sie do wykorzstania obecnie istniejących rozwiązań. Porównania wyników jakie uzyskano oraz próbie modyfikacji modeli w celu poprawy wyników dla wyspecyfikowanego typu zagadnienia. Próby będą dotyczyły zmiany architektury sieci, trybów uczenia, zmiany funkcji aktywacji oraz modyfikacji optymalizowanej funkcji strat. Wykorzystane zostaną nie tylko frameworki takiego jak <br>- [[1] reid-strong-baseline](https://github.com/michuanhaohao/reid-strong-baseline) <br> czy <br> - [[4] Torchreid: Library for Deep Learning Person Re_Identyfication in Pytoch](https://arxiv.org/pdf/1910.10093.pdf)\n",
        "<br>\n",
        "ale również techniki takie jak finetuning.\n",
        "<br><br>\n",
        "Drugie założenie specyfikuje problem. Nadaje kierunek w jakim ma podążać optymalizaja wyników oraz cech sieci. W tym dostosowanie sieci do pracy w trybie \"real time\" co wymusza skracanie czasu obliczeń i zmiejszanie sieci przy zachowaniu tego samego poziomu dokładności. Powody wybieranych rozwiązań upatrywać należy właśnie w spełnieniu tego założenia.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxMJLXm63FqE",
        "colab_type": "text"
      },
      "source": [
        "### 3 Wybór systemów do reidentyfikacji"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GNv_JsSLPzS",
        "colab_type": "text"
      },
      "source": [
        "W badaniach wykorzystano dwa powszechnie używane framework-i:\n",
        "- [[1] reid-strong-baseline](https://github.com/michuanhaohao/reid-strong-baseline) <br>\n",
        "Zgodnie z opisem je to \n",
        "- [[5] Torchreid: Library for Deep Learning Person Re_Identyfication in Pytoch](https://github.com/KaiyangZhou/deep-person-reid/tree/master/projects)\n",
        "<br> Jest to framework napisany z użyciem PyTorch i jest kompleksowym narzędziem do tworzenia i porównywania sieci stworzonych do reidentyfikacji sieci. Zawiera wiele baz danych.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wJwM5UffVB_",
        "colab_type": "text"
      },
      "source": [
        "## [3. DataSets]()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZFJA2CxWRHy",
        "colab_type": "text"
      },
      "source": [
        "Lista dostępnych datasetów obsługiwanych przez - [[5] Torchreid: Library for Deep Learning Person Re_Identyfication in Pytoch](https://github.com/KaiyangZhou/deep-person-reid/tree/master/projects) jest bogata i zawiera:\n",
        "<br>\n",
        "<br>\n",
        "a. Datasety ze zdjęciami:\n",
        "- Market1501 [[7] Market1501 - pdf](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.pdf)\n",
        "- CUHK03\n",
        "- DukeMTMC-reID\n",
        "- MSMT17\n",
        "- VIPeR\n",
        "- GRID\n",
        "- CUHK01\n",
        "- SenseReID\n",
        "- QMUL-iLIDS\n",
        "- PRID\n",
        "<br><br>\n",
        "\n",
        "b. Datasety ze nagraniami video:\n",
        "- MARS\n",
        "- iLIDS-VID\n",
        "- PRID2011\n",
        "- DukeMTMC-VideoReID\n",
        "<br><br>\n",
        "\n",
        "Dwa najczęściej wykorzystywane w tego typu zadaniach datasety to:\n",
        "- Market1501 [[7] Market1501 - pdf](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.pdf)\n",
        "- DukeMTMC-reID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erIvXPUecM5Q",
        "colab_type": "text"
      },
      "source": [
        "### Market1501\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxws4UY3vd1y",
        "colab_type": "text"
      },
      "source": [
        "<br><br>\n",
        "<img src=\"https://github.com/tomektarabasz/Praca_Dyplomowa_Tomasz_Tarabasz/blob/master/img/market_1.png?raw=true\" alt=\"drawing\" width=\"400\"/>\n",
        "<br><br>\n",
        "<br><br>\n",
        "<img src=\"https://github.com/tomektarabasz/Praca_Dyplomowa_Tomasz_Tarabasz/blob/master/img/market_2.png?raw=true\" alt=\"drawing\" width=\"400\"/>\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hKb0XQwZas1",
        "colab_type": "text"
      },
      "source": [
        "#### Duke MTMC\n",
        "\"Duke MTMC (Multi-Target, Multi-Camera) to zbiór danych z nagrań wideo z monitoringu zrobionych na kampusie Duke University w 2014 roku i jest używany do badań i rozwoju systemów śledzenia wideo, ponownej identyfikacji osób i rozpoznawania twarzy o niskiej rozdzielczości.\n",
        "\n",
        "Zestaw danych zawiera ponad 14 godzin zsynchronizowanego obrazu wideo z 8 kamer przy 1080p i 60 FPS, z ponad 2 milionami klatek 2000 uczniów idących do i z zajęć. Osiem kamer monitorujących rozmieszczonych na terenie kampusu zostało specjalnie skonfigurowanych do rejestrowania uczniów „w okresach między wykładami, kiedy ruch pieszy jest duży” to opis zbioru z pracy [[10] Duke MTMC dataset](https://megapixels.cc/duke_mtmc/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiBz4O6SmqGu",
        "colab_type": "text"
      },
      "source": [
        "### Własny zbiór danych"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTCYR0D7my6-",
        "colab_type": "text"
      },
      "source": [
        "W celu weryfikacji przydatności modeli do wykorzystania w zadaniu śledzenia osób na nagraniach wideo, stworzono własny zbiór danych. Zbór ten pełnił rolę zbioru porównawczego do oceny jakość tworzenia embedingów z nagrań kamery 360. Posłużono się modelem kamery powszechnie stosowanym w punktach sprzedaży lokalizowanych w centrach handlowych oraz samodzielnych salonach obsługi klientów.\n",
        "\n",
        "Zbiór wygenrowano z nagrania\n",
        "<br><br>\n",
        "<img src=\"https://github.com/tomektarabasz/Praca_Dyplomowa_Tomasz_Tarabasz/blob/master/img/my_dataset1.png?raw=true\" alt=\"drawing\" width=\"400\"/>\n",
        "<br>\n",
        "\n",
        "[Link do autorskiego nagrania](https://drive.google.com/file/d/1-0vgAB7ujrl-55ZU-8IpkaS_7wqbt9Mq/view?usp=sharing)\n",
        "\n",
        "<br>\n",
        "\n",
        "Do wyodrębnienia obiektów na nagraniu wykorzystano framework YOLO5 [[11]](https://github.com/ultralytics/yolov5/) z obektów wyekstrachowanych z nagrania wybrano jedynie postać ludzką. Z nagrania o długości 1min26s uzyskano 1203 wycięte fragmenty z tą samą postacią w różnych pozach. Przykładowe wycięte obrazy:\n",
        "<br><br>\n",
        "<img src=\"https://github.com/tomektarabasz/Praca_Dyplomowa_Tomasz_Tarabasz/blob/master/img/me_1.jpg?raw=true\" alt=\"drawing\" width=\"400\"/>\n",
        "<br>\n",
        "<img src=\"https://github.com/tomektarabasz/Praca_Dyplomowa_Tomasz_Tarabasz/blob/master/img/me_2.jpg?raw=true\" alt=\"drawing\" width=\"400\"/>\n",
        "<br>\n",
        "<img src=\"https://github.com/tomektarabasz/Praca_Dyplomowa_Tomasz_Tarabasz/blob/master/img/me_3.jpg?raw=true\" alt=\"drawing\" width=\"400\"/>\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBZFH-DYP8mr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e91a2058-c46f-4510-832d-fe89176fdbd0"
      },
      "source": [
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  import os\n",
        "  import shutil\n",
        "  photo_path = '/content/gdrive/My Drive/EMBEDINGS_TESTS/TT_cropped'\n",
        "  os.path.exists(photo_path)\n",
        "\n",
        "  files = os.listdir(photo_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/4AGm-qioR3-orWooxLxbCgZuuMeP36JWp_5H3iSaVZQTKPyk4WYi6Bk\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waFL5tRYQVuE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf803293-c042-4c02-c49e-eb6c1b925dcd"
      },
      "source": [
        "files.__len__()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1203"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYyX8Yramd7u",
        "colab_type": "text"
      },
      "source": [
        "### Podsumowanie"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk35pbT7cJUR",
        "colab_type": "text"
      },
      "source": [
        "W tej pracy wykorzystany zostanie jedynie dataset Maret1501. Zdecydowano o użyciu jednego zbioru danych z powodu ograniczeń sprzętowych oraz by w procesie porównawczym wyeliminować wpływ doboru danych uczących, które przy wielu zbiorach danych w połaczeniu z ograniczoną ilością epoko uczenia znacząco wpływałaby na ostateczny wynik modeli.\n",
        "<br><br>\n",
        "Szczegółwy opis zbioru danych znajduje się w pracy [[7] Market1501 - pdf](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.pdf). Analiz zbioru danych zawarta jest w notebooku ... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHkAaNHDcDr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJvqLrMrHxum",
        "colab_type": "text"
      },
      "source": [
        "## [4. Modele](notebooks/02_ReId_BasseLine.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uetywebljy0g",
        "colab_type": "text"
      },
      "source": [
        "- [ReId BaseLine](notebooks/02_ReId_BasseLine.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTFXqK2bj0y3",
        "colab_type": "text"
      },
      "source": [
        "- [EfficientNet](notebooks/02_ReId_BasseLine.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iULHWjzd8im",
        "colab_type": "text"
      },
      "source": [
        "## 5. Rozwiązania optymalizujące proces trenowania."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7ejVvXFUB-0",
        "colab_type": "text"
      },
      "source": [
        "Zmiany poprawiające proces uczenia zaimpementowane w [[1] reid-strong-baseline -github](https://github.com/michuanhaohao/reid-strong-baseline) zostały zaprezentowane na grafice umieszczonej poniżej.\n",
        "<br><br>\n",
        "<img src=\"https://github.com/tomektarabasz/Praca_Dyplomowa_Tomasz_Tarabasz/blob/master/img/reidTricks.jpg?raw=true\" alt=\"drawing\" width=\"400\"/>\n",
        "<br><br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptIMo97ueGHH",
        "colab_type": "text"
      },
      "source": [
        "### 5.1 Rozgrzewanie kroku uczenia (Warmup Learning Rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-9VsCY2emKh",
        "colab_type": "text"
      },
      "source": [
        "### 5.2 Losowo wycinanie (Random Erasing Augumentation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlhJqJ95emHe",
        "colab_type": "text"
      },
      "source": [
        "### 5.3 (Label Smoothing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRmLz57WemDu",
        "colab_type": "text"
      },
      "source": [
        "### 5.4 (Last Stride)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcrNO1Kvelw2",
        "colab_type": "text"
      },
      "source": [
        "### 5.5 BBNeck"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73nPWLDRfet6",
        "colab_type": "text"
      },
      "source": [
        "### 5.6 Center Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbpfy45Yg-_l",
        "colab_type": "text"
      },
      "source": [
        "## 6. Rozwiązanie bazowe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwBwo6PtHUoz",
        "colab_type": "text"
      },
      "source": [
        "## [7. Modyfikacje]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Fy4hUCekrrN",
        "colab_type": "text"
      },
      "source": [
        "### 7.1 Użycie jako sieci bazowej sieci EfficientNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p520kf8kyKE",
        "colab_type": "text"
      },
      "source": [
        "### [[2] EfficientNet-PyTorch](https://github.com/lukemelas/EfficientNet-PyTorch?fbclid=IwAR28bdEvf05yCsMdE7ByUP5z-6EyRmadPp5EoyLd57nahLfsikDEuiIT7eU)\n",
        "\n",
        "to repozytorium z implementacja sieci EficientNet z użyciem biblioteki PyTorch.\n",
        "\n",
        "Repozytorium zawiera wytrenowane model na z bazie obrazów ImageNet. Posiada również zaimplementowane metody do ewaluacji wytrenowanej sieci na własnym zbiorze danych.\n",
        "\n",
        "###  Opis sieci EfficienNet:\n",
        "\n",
        "Zgodnie z opisem zawartym w repozytorium EfficientNet to model należący do grupy modeli dedykowanych klasyfikacji obrazów. Uzyskał on, na moment pisanie tej pracy, najlepsze wyniki w tej grupie zadań. Ponad to jest mniejszym i szybszym modelem niz jego poprzednicy. Jest wzorowany na modelach AutoML oraz  Compound Scaling.\n",
        "\n",
        "W szczególności model EfficientNet-B0 jest modelem o rozmiarze odpowiadającym rozwiązaniom mobilnym, który czerpie z AutoML Mobile framework.\n",
        "\n",
        "W dokumencie zaprezentowano wykresy prezentujące ilość parametrów dla kilku z wiodących rozwiązań w porównaniu do EfficientNet\n",
        "\n",
        "<br><br>\n",
        "<img src=\"https://github.com/tomektarabasz/Praca_Dyplomowa_Tomasz_Tarabasz/blob/master/img/EfficientNetPorownanie.jpg?raw=true\" alt=\"drawing\" width=\"400\"/>\n",
        "<br><br>\n",
        "\n",
        "Autorzy rozwiązania wymieniają osiągnięcia modelu jako: \n",
        " - Przy wysokim poziomie dokładności wyników sieć EfficientNet-B7 osiągneło najlepszy wynik o poziomie 84.4% dla \"top-1\" oraz 97.1% wśród pięciu najlepszych wyników (\"top-5\") na zbiorze danych ImageNet z 66 milionami parametów i 37B FLOPS. Jest to model 8.4 razy mniejszy i 6.1 razy szybszy na CPU od swojego poprzednika Gpipe ([[8] GPipe - pdf](https://arxiv.org/pdf/1811.06965.pdf)).\n",
        "\n",
        " - Przy średnim poziomie dokładności wyników, model EfficientNet-B1 jest 7,6 razy mniejszy i 5,7 razy szybszy na CPU od ResNet-152 ([[9] Deep Residual Learning for Image Recognition - pdf](https://arxiv.org/pdf/1512.03385.pdf)) z porównywanym wynikiem dokładności dla ImageNet.\n",
        "\n",
        " - W porównaniu z powszechnie używanym ResNet-50 model EfficientNet-B4 porawia wyniki dla \"top-1\" rezultatów, czyli najbardziej prawdopodobnej klasyfikacji, o 6.3% (z poziomu 76,3% do 82.6%) dla tego samego poziomu FLOPS świadczącym o szybkości uzyskania predykcji.\n",
        "\n",
        "Są to powody dla których zdecydowano o próbie wykorzystania tej architektury jako \"BackBone\", czyli ekstraktora cech w problemie reidentyfikacji osób. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQYtOJWSkmsG",
        "colab_type": "text"
      },
      "source": [
        "### 7.2 Wprowadzenie funkcji Swish jako funkcji aktywacji"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho2xRraRB0F4",
        "colab_type": "text"
      },
      "source": [
        "Funkcja Swish zdefiniowano jako: \n",
        "<br><br>\n",
        "$\\text{Swish}(x) = x \\cdot (1 - {e}^{-x})^{-1}$\n",
        "<br><br>\n",
        "<img src=\"https://github.com/tomektarabasz/Praca_Dyplomowa_Tomasz_Tarabasz/blob/master/img/Swish_plot.jpg?raw=true\" alt=\"drawing\" width=\"400\"/>\n",
        "<br><br>\n",
        "\n",
        "Zgodnie z [[3] Why Swish could perform better than ReLu](https://www.machinecurve.com/index.php/2019/05/30/why-swish-could-perform-better-than-relu/?fbclid=IwAR1muCJQpjU4M4dHcCx2wj3PAtYtL6rpEi5-aU9LSjrkrBONQXTqScR2AtU) taka postać funkcji aktywacji może poprawić wyniki poprzez:\n",
        " - Dla dużych wartości ujemnych, tak jak funkcja ReLu zeruje takie aktywacje.\n",
        " - Dla wartości dodatnich aktywacji funkcja Swish zachowuje się podobnie do ReLu i nie ograniczna tych wartości \"od góry\"\n",
        " - W okolicjach zera funkcja jest różniczkowalna i nie zawiera nielinowości\n",
        " - Małe wartości ujemne nie są zerowane co może pomóc w doszkalaniu modelu w procesie szukania subtelnych cech głębokich."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sogNxIKlEJ8",
        "colab_type": "text"
      },
      "source": [
        "## 8. Porównanie modeli."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fk2mqbElPDB",
        "colab_type": "text"
      },
      "source": [
        "### 8.1 Mean Avarage Precision (mAP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPs1G5oflVaj",
        "colab_type": "text"
      },
      "source": [
        "### 8.2 Czas przetwarzania obrazu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22CiFYcmIVC9",
        "colab_type": "text"
      },
      "source": [
        "## 10. Bibliografia\n",
        "- [[1] reid-strong-baseline -github](https://github.com/michuanhaohao/reid-strong-baseline)\n",
        "- [[2] EfficientNet-PyTorch](https://github.com/lukemelas/EfficientNet-PyTorch?fbclid=IwAR28bdEvf05yCsMdE7ByUP5z-6EyRmadPp5EoyLd57nahLfsikDEuiIT7eU)\n",
        "- [[3] Why Swish could perform better than ReLu](https://www.machinecurve.com/index.php/2019/05/30/why-swish-could-perform-better-than-relu/?fbclid=IwAR1muCJQpjU4M4dHcCx2wj3PAtYtL6rpEi5-aU9LSjrkrBONQXTqScR2AtU)\n",
        "- [[4] Torchreid: Library for Deep Learning Person Re_Identyfication in Pytoch](https://arxiv.org/pdf/1910.10093.pdf)\n",
        "- [[5] Torchreid: Library for Deep Learning Person Re_Identyfication in Pytoch - github project](https://github.com/KaiyangZhou/deep-person-reid/tree/master/projects)\n",
        "- [[6] reid-strong-baseline - pdf](https://arxiv.org/pdf/1903.07071.pdf)\n",
        "- [[7] Market1501 - pdf](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.pdf)\n",
        "- [[8] GPipe - pdf](https://arxiv.org/pdf/1811.06965.pdf)\n",
        "- [[9] Deep Residual Learning for Image Recognition - pdf](https://arxiv.org/pdf/1512.03385.pdf)\n",
        "- [[10] Duke MTMC dataset](https://megapixels.cc/duke_mtmc/)\n",
        "- [[11] YOLO5](https://github.com/ultralytics/yolov5/)\n"
      ]
    }
  ]
}