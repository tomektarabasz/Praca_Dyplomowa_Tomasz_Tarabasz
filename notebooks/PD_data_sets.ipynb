{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PD_data_sets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpTaN2wARjNl",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "\n",
        "| [1. Streszczenie, Wstęp, Wybór systemów do reidentyfikacji](PD_s_w_w.ipynb) | [5. Modele](PD_modele.ipynb) >\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wJwM5UffVB_",
        "colab_type": "text"
      },
      "source": [
        "## [4. DataSets]()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZFJA2CxWRHy",
        "colab_type": "text"
      },
      "source": [
        "Lista dostępnych datasetów obsługiwanych przez - [[5] Torchreid: Library for Deep Learning Person Re_Identyfication in Pytoch](https://github.com/KaiyangZhou/deep-person-reid/tree/master/projects) jest bogata i zawiera:\n",
        "<br>\n",
        "<br>\n",
        "a. Datasety zawierajace pojedyńcze ujęcia:\n",
        "- Market1501 [[7] Market1501 - pdf](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.pdf)\n",
        "- CUHK03\n",
        "- DukeMTMC-reID\n",
        "- MSMT17\n",
        "- VIPeR\n",
        "- GRID\n",
        "- CUHK01\n",
        "- SenseReID\n",
        "- QMUL-iLIDS\n",
        "- PRID\n",
        "<br><br>\n",
        "\n",
        "b. Datasety ze nagraniami video:\n",
        "- MARS\n",
        "- iLIDS-VID\n",
        "- PRID2011\n",
        "- DukeMTMC-VideoReID\n",
        "<br><br>\n",
        "\n",
        "Dwa najczęściej wykorzystywane w tego typu zadaniach datasety to:\n",
        "- Market1501 [[7] Market1501 - pdf](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.pdf)\n",
        "- DukeMTMC-reID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erIvXPUecM5Q",
        "colab_type": "text"
      },
      "source": [
        "### [4.1 Market1501]()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wP9O4ueWX6Z",
        "colab_type": "text"
      },
      "source": [
        "To jeden z najczęściej wykorzystywanych zbiór danych do trenowania systemów reidentyfikacji osób. Jak twierdzą twórcy: \"To nowy wysokiej jakości dataset do reidetnyfikacji osób nazwanym 'Market-1501'. Ogólenie dotychczasowe zbory:\n",
        " - są ograniczone w skalowaniu\n",
        " - posiadają ręcznie zaznaczone obramowania, które nie są dostępne w realnych ustawieniach\n",
        " - posiadają jedynie jeden obraz wzorcowy dla każdej z osób z zestawu instancji do wyszukiwania\n",
        "\n",
        "W celu wyeliminowania tych problemów zbór Market-1501 posiada trzy cechy. Pierwsza z nich to 32 000 zidentyfikowanych obramowań, ponad to ponad 500 tysięcy obrazów, które tworzą największy zbiór osób do trenowania modeli z zakresu zadań reidentyfikacji. Druga to stworzenie obrazów został stworzony przez 'Deformable Part Model' (DPM) jako detektor pieszych. Trzecia to stowrzenie zbioru w otwartym systemie dzięki któremu, któremu każda osoba zawiera wiele ujęć z wielu kamer.\"\n",
        "\n",
        "Przykład ze zbioru Market-1501 zaprezentowano poniżej."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxws4UY3vd1y",
        "colab_type": "text"
      },
      "source": [
        "<br><br>\n",
        "<img src=\"https://github.com/tomektarabasz/Praca_Dyplomowa_Tomasz_Tarabasz/blob/master/img/market_1.png?raw=true\" alt=\"drawing\" width=\"400\"/>\n",
        "<img src=\"https://github.com/tomektarabasz/Praca_Dyplomowa_Tomasz_Tarabasz/blob/master/img/market_2.png?raw=true\" alt=\"drawing\" width=\"125\"/>\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qcWalcEYr0S",
        "colab_type": "text"
      },
      "source": [
        "### [4.2 Duke MTMC]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hKb0XQwZas1",
        "colab_type": "text"
      },
      "source": [
        "\"Duke MTMC (Multi-Target, Multi-Camera) to zbiór danych z nagrań wideo z monitoringu zrobionych na kampusie Duke University w 2014 roku i jest używany do badań i rozwoju systemów śledzenia wideo, ponownej identyfikacji osób i rozpoznawania twarzy o niskiej rozdzielczości.\n",
        "\n",
        "Zestaw danych zawiera ponad 14 godzin zsynchronizowanego obrazu wideo z 8 kamer przy 1080p i 60 FPS, z ponad 2 milionami klatek 2000 uczniów idących do i z zajęć. Osiem kamer monitorujących rozmieszczonych na terenie kampusu zostało specjalnie skonfigurowanych do rejestrowania uczniów „w okresach między wykładami, kiedy ruch pieszy jest duży” to opis zbioru z pracy [[10] Duke MTMC dataset](https://megapixels.cc/duke_mtmc/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiBz4O6SmqGu",
        "colab_type": "text"
      },
      "source": [
        "### [4.3 Własny zbiór danych]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTCYR0D7my6-",
        "colab_type": "text"
      },
      "source": [
        "W celu weryfikacji przydatności modeli do wykorzystania w zadaniu śledzenia osób na nagraniach wideo, stworzono własny zbiór danych. Zbór ten pełnił rolę zbioru porównawczego do oceny jakość tworzenia embedingów z nagrań kamery 360. Posłużono się modelem kamery powszechnie stosowanym w punktach sprzedaży lokalizowanych w centrach handlowych oraz samodzielnych salonach obsługi klientów.\n",
        "\n",
        "Zbiór wygenrowano z nagrania\n",
        "<br><br>\n",
        "<img src=\"https://github.com/tomektarabasz/Praca_Dyplomowa_Tomasz_Tarabasz/blob/master/img/my_dataset1.png?raw=true\" alt=\"drawing\" width=\"800\"/>\n",
        "<br>\n",
        "\n",
        "[Link do autorskiego nagrania](https://drive.google.com/file/d/1-0vgAB7ujrl-55ZU-8IpkaS_7wqbt9Mq/view?usp=sharing)\n",
        "\n",
        "<br>\n",
        "\n",
        "Do wyodrębnienia obiektów na nagraniu wykorzystano framework YOLO5 [[11]](https://github.com/ultralytics/yolov5/) z obektów wyekstrachowanych z nagrania wybrano jedną postać ludzką. Z nagrania o długości <b>1min26s</b> uzyskano <b>1203</b> wycięte fragmenty z tą samą postacią w różnych pozach. Przykładowe wycięte obrazy:\n",
        "<br><br>\n",
        "<img src=\"https://github.com/tomektarabasz/Praca_Dyplomowa_Tomasz_Tarabasz/blob/master/img/me_1.jpg?raw=true\" alt=\"drawing\" width=\"100\"/>\n",
        "<img src=\"https://github.com/tomektarabasz/Praca_Dyplomowa_Tomasz_Tarabasz/blob/master/img/me_2.jpg?raw=true\" alt=\"drawing\" width=\"100\"/>\n",
        "<img src=\"https://github.com/tomektarabasz/Praca_Dyplomowa_Tomasz_Tarabasz/blob/master/img/me_3.jpg?raw=true\" alt=\"drawing\" width=\"100\"/>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz1Swzu0Slky",
        "colab_type": "text"
      },
      "source": [
        "### [4.4 Zbiór wygenerowany z nagrania pobranego z YouTube]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdifF8EMSyod",
        "colab_type": "text"
      },
      "source": [
        "Zbiór wygenerowano z nagrania umieszczonego poniżej.\n",
        "<br><br>\n",
        "<img src=\"https://github.com/tomektarabasz/Praca_Dyplomowa_Tomasz_Tarabasz/blob/master/img/record_movie.png?raw=true\" alt=\"drawing\" width=\"800\"/>\n",
        "<br>\n",
        "\n",
        "[Link do nagrania](https://drive.google.com/file/d/1P2AdTCr0f2htDGtW2rmq5qMQ1L3uf3fG/view?usp=sharing)\n",
        "\n",
        "Korzystając z frameworku YOLO5 zmodyfikowanym na potrzeby tej pracy, jedynie do detekcji postaci ludzkich, wygenerowano dataset z wyciętymi osobami z nagrania. Posłużą one do stworzenia embedingów i wyszukania tej samej osoby z kolejnych klatek nagrania. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDpfqRTmAdZV",
        "colab_type": "text"
      },
      "source": [
        "### [4.5 Komórki pomocniczne]()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBZFH-DYP8mr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37da8603-5737-4b80-b6ce-4b6cb841a4fd"
      },
      "source": [
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  import os\n",
        "  import shutil\n",
        "  photo_path = '/content/gdrive/My Drive/EMBEDINGS_TESTS/TT_cropped'\n",
        "  os.path.exists(photo_path)\n",
        "\n",
        "  files = os.listdir(photo_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waFL5tRYQVuE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "229c1f4e-769a-49e1-9edd-0448cc91b079"
      },
      "source": [
        "files.__len__()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1203"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYyX8Yramd7u",
        "colab_type": "text"
      },
      "source": [
        "### [4.6 Podsumowanie wyboru dataset-u]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S73m9zKYSSaB",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk35pbT7cJUR",
        "colab_type": "text"
      },
      "source": [
        "W tej pracy wykorzystany zostanie jedynie zewnętrzny dataset Maret1501 oraz własny dataset porównawczy. Zdecydowano o użyciu jednego zbioru danych z powodu ograniczeń sprzętowych oraz by w procesie porównawczym wyeliminować wpływ doboru danych uczących, które przy wielu zbiorach danych w połaczeniu z ograniczoną ilością epoko uczenia znacząco wpływałaby na ostateczny wynik modeli.\n",
        "<br><br>\n",
        "Szczegółwy opis zbioru danych znajduje się w pracy [[7] Market1501 - pdf](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Scalable_Person_Re-Identification_ICCV_2015_paper.pdf). Analiz zbioru danych zawarta jest w notebooku ...\n",
        "<br><br>\n",
        "Własny dataset zostanie wykorzystany jedynie w celu wyciągnięcia wniosków o jakości modeli w dwóch wybranych kryteriach:\n",
        "- zbierzności embedinów dla datasetu złożonego z jednej postaci\n",
        "- czasu przetwarzania.\n",
        "\n",
        "\n",
        "Parameter czasu przetwarzania jest szczególnie istotny pod względem wykorzystania w systemie śledzenia i identyfikacji rozpoznanych sylwetek ludzkich\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hev6frgKRmRA",
        "colab_type": "text"
      },
      "source": [
        "## Nawigacja"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l6DBg574WZay"
      },
      "source": [
        "<br>\n",
        "\n",
        "| [1. Streszczenie, Wstęp, Wybór systemów do reidentyfikacji](PD_s_w_w.ipynb) | [5. Modele](PD_modele.ipynb) >\n"
      ]
    }
  ]
}